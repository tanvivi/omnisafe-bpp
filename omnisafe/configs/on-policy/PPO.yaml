# Copyright 2023 OmniSafe Team. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

defaults:
  # seed for random number generator
  seed: 0
  # training configurations
  train_cfgs:
    # device to use for training, options: cpu, cuda, cuda:0, cuda:0,1, etc.
    device: cuda:0
    # number of threads for torch
    torch_threads: 16
    # number of vectorized environments
    vector_env_nums: 1
    # number of parallel agent, similar to a3c
    parallel: 1
    # total number of steps to train
    total_steps: 10000000
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 4096
    # number of iterations to update the policy
    update_iters: 5
    # batch size for each iteration
    batch_size: 128
    # target kl divergence
    target_kl: 0.1
    # entropy coefficient
    entropy_coef: 0.05
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: False
    # normalize observation
    obs_normalize: False
    # early stop when kl divergence is bigger than target kl
    kl_early_stop: True
    # use max gradient norm
    use_max_grad_norm: True
    # max gradient norm
    max_grad_norm: 0.5 # only works when modified here
    # use critic norm
    use_critic_norm: True
    # critic norm coefficient
    critic_norm_coef: 0.001
    # reward discount factor
    gamma: 1.00
    # cost discount factor
    cost_gamma: 0.99
    # lambda for gae
    lam: 0.95
    # lambda for cost gae
    lam_c: 0.95
    # clip ratio
    clip: 0.2
    # advantage estimation method, options: gae, retrace
    adv_estimation_method: gae
    # standardize reward advantage
    standardized_rew_adv: True
    # standardize cost advantage
    standardized_cost_adv: True
    # penalty coefficient
    penalty_coef: 0.0
    # use cost
    use_cost: False
  # logger configurations
  logger_cfgs:
    # use wandb for logging
    use_wandb: True
    # wandb project name
    wandb_project: bin-selection
    # wandb run name
    wandb_run_name: bins_2_lr_1e-5_v1
    # use tensorboard for logging
    use_tensorboard: False
    # save model frequency
    save_model_freq: 100
    # save logger path
    log_dir: "./runs"
    # save model path
    window_lens: 100
  # model configurations
  model_cfgs:
    device: cuda:0 # for device setting, only set here works
    # bin_state_dim
    bin_state_dim: 5
    # bin size
    bin_size: [10, 10, 10]
    # embedding size
    embed_size: 64
    # number of layers
    num_layers: 3
    # number of heads
    heads: 2
    # dropout rate
    dropout: 0.1
    # whether to use padding mask
    padding_mask: True
    # share network between actor and critic
    share_net: null
    # weight initialization mode
    weight_initialization_mode: "kaiming_uniform"
    # actor type, options: gaussian, gaussian_learning
    actor_type: categorical
    # linear learning rate decay
    linear_lr_decay: True
    # exploration noise anneal
    exploration_noise_anneal: False
    # std upper bound, and lower bound
    std_range: [0.5, 0.1]
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [64, 64]
      # activation function
      activation: tanh
      # out_activation: tanh
      # learning rate
      lr: 0.00005
    critic:
      # hidden layer sizes
      hidden_sizes: [128, 128]
      # activation function
      activation: tanh
      # learning rate
      lr: 0.00005

ShadowHandCatchOver2UnderarmSafeFinger:
  # training configurations
  train_cfgs:
    # number of vectorized environments
    vector_env_nums: 256
    # total number of steps to train
    total_steps: 100000000
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 38400
    # number of iterations to update the policy
    update_iters: 8
    # batch size for each iteration
    batch_size: 8192
    # target kl divergence
    target_kl: 0.016
    # max gradient norm
    max_grad_norm: 1.0
    # use critic norm
    use_critic_norm: False
    # reward discount factor
    gamma: 0.96
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: False
    # normalize observation
    obs_normalize: False
  # model configurations
  model_cfgs:
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
    critic:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
      # learning rate
      lr: 0.0006

ShadowHandOverSafeFinger:
  # training configurations
  train_cfgs:
    # number of vectorized environments
    vector_env_nums: 256
    # total number of steps to train
    total_steps: 100000000
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 38400
    # number of iterations to update the policy
    update_iters: 8
    # batch size for each iteration
    batch_size: 8192
    # target kl divergence
    target_kl: 0.016
    # max gradient norm
    max_grad_norm: 1.0
    # use critic norm
    use_critic_norm: False
    # reward discount factor
    gamma: 0.96
    # normalize observation
    obs_normalize: False
  # model configurations
  model_cfgs:
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
    critic:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
      # learning rate
      lr: 0.0006

ShadowHandCatchOver2UnderarmSafeJoint:
  # training configurations
  train_cfgs:
    # number of vectorized environments
    vector_env_nums: 256
    # total number of steps to train
    total_steps: 100000000
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 38400
    # number of iterations to update the policy
    update_iters: 8
    # batch size for each iteration
    batch_size: 8192
    # target kl divergence
    target_kl: 0.016
    # max gradient norm
    max_grad_norm: 1.0
    # use critic norm
    use_critic_norm: False
    # reward discount factor
    gamma: 0.96
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: False
    # normalize observation
    obs_normalize: False
  # model configurations
  model_cfgs:
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
    critic:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
      # learning rate
      lr: 0.0006

ShadowHandOverSafeJoint:
  # training configurations
  train_cfgs:
    # number of vectorized environments
    vector_env_nums: 256
    # total number of steps to train
    total_steps: 100000000
  # algorithm configurations
  algo_cfgs:
    # number of steps to update the policy
    steps_per_epoch: 38400
    # number of iterations to update the policy
    update_iters: 8
    # batch size for each iteration
    batch_size: 8192
    # target kl divergence
    target_kl: 0.016
    # max gradient norm
    max_grad_norm: 1.0
    # use critic norm
    use_critic_norm: False
    # reward discount factor
    gamma: 0.96
    # normalize observation
    obs_normalize: False
  # model configurations
  model_cfgs:
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
    critic:
      # hidden layer sizes
      hidden_sizes: [1024, 1024, 512]
      # learning rate
      lr: 0.0006

SafeMetaDrive:
  # training configurations
  train_cfgs:
    # total number of steps to train
    total_steps: 500000
  # logger configurations
  logger_cfgs:
    # save model frequency
    save_model_freq: 15
  # algorithm configurations
  algo_cfgs:
    # batch size for each iteration
    batch_size: 256
    # entropy coefficient
    entropy_coef: 0.01
    # number of steps to update the policy
    steps_per_epoch: 3000
    # number of iterations to update the policy
    update_iters: 40
    # normalize reward
    reward_normalize: False
    # normalize cost
    cost_normalize: True
    # normalize observation
    obs_normalize: False
    # max gradient norm
    max_grad_norm: 0.05
    # penalty coefficient
    penalty_coef: 1.0
  # model configurations
  model_cfgs:
    # actor network configurations
    actor:
      # hidden layer sizes
      hidden_sizes: [256, 256]
      # learning rate
      lr: 0.00005
    # critic network configurations
    critic:
      # hidden layer sizes
      hidden_sizes: [256, 256]
      # learning rate
      lr: 0.00005
  # environment specific configurations
  env_cfgs:
  # safe meta drive configurations. More details refer to https://github.com/decisionforce/EGPO
    meta_drive_config:
      # max iterations of interactions
      horizon: 1500
      # whether to use random traffic
      random_traffic: False
      # the penalty when crash into other vehicles
      crash_vehicle_penalty: 1.
      # the penalty when crash into other objects
      crash_object_penalty: 0.5
      # the penalty when out of road
      out_of_road_penalty: 1.
